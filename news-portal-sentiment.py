# -*- coding: utf-8 -*-
"""Latihan Piton Data Mining

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H4nNsAGJMvhPM69TLpe4Bll6f79GHKsR

**Menggunakan NLTK**
"""

!pip install nltk

import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

nltk.download('punkt')

# Contoh dokumen teks
text = """
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse eleifend, arcu nec sollicitudin dictum, urna lorem dictum dolor, lobortis blandit ipsum odio eu tortor. Nam cursus turpis nec quam mollis, ac fringilla enim scelerisque. Nam dignissim sapien faucibus est bibendum suscipit. Maecenas sapien urna, congue eget metus in, finibus vestibulum nunc. Duis ante metus, sodales ac lacus et, imperdiet semper massa. Donec vestibulum odio lorem, eget euismod erat pulvinar vel. Nam at justo quis urna porta commodo eget nec sapien. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aliquam consectetur tortor quis libero pulvinar pulvinar. Nam a leo euismod, fermentum diam vel, lacinia lorem. Nam vel erat fringilla, lobortis erat vel, tristique mauris. Quisque elementum nibh in lacus blandit vulputate. Nam placerat suscipit erat, quis bibendum augue laoreet quis. Praesent tristique pretium magna, sed interdum ipsum mattis eu. Nulla sodales semper viverra.
Donec ornare sem sit amet commodo suscipit. Suspendisse et quam nec massa consectetur iaculis in at felis. Fusce at erat lacinia, ultrices ligula nec, suscipit nisi. Sed tristique augue ac rhoncus gravida. Integer rhoncus elit ante, vel aliquam massa tincidunt in. Ut euismod bibendum risus at sagittis. Suspendisse eget sem id orci finibus convallis eget eu dui. Morbi sed nunc quis velit bibendum semper vitae nec tortor. Curabitur quis nisi ac quam ultrices interdum. Curabitur ac ultricies urna. Nulla maximus dui eu luctus tempor. Morbi mollis risus eget mi ornare, eu lobortis lacus sollicitudin. Integer sollicitudin sit amet mauris nec aliquet. Cras placerat libero et ex condimentum, vitae condimentum erat pharetra.
Integer id lectus quis purus accumsan lacinia. Pellentesque ultrices, nunc sit amet suscipit imperdiet, quam mi vulputate nisl, sed placerat turpis est non sapien. Nam eu ante sit amet ante tristique luctus vel accumsan nisl. Aenean elementum lectus at vestibulum hendrerit. In pulvinar mattis efficitur. Donec non dignissim enim, et cursus quam. Nulla ullamcorper dapibus elit, lacinia fringilla quam convallis iaculis. Etiam sollicitudin elementum ligula a ultrices. Aenean ultricies, justo vitae luctus tempor, mauris turpis dictum enim, eget dictum ante elit at nulla. Proin in mauris ac orci tristique auctor at sed purus. Vivamus luctus magna sapien, vel dapibus leo condimentum vitae. Nullam consequat rhoncus felis at sollicitudin.
Nulla facilisi. Vivamus semper, odio at tempus blandit, ligula est aliquet nulla, quis viverra nulla felis et ligula. Aliquam erat volutpat. Aenean eget sagittis diam, quis imperdiet lectus. In efficitur orci justo, nec laoreet odio fermentum a. Integer volutpat rutrum dui, id varius enim tincidunt id. Duis eleifend venenatis suscipit. Vestibulum hendrerit faucibus sem, at accumsan libero ornare sed.
Praesent malesuada consequat mauris eget fermentum. Maecenas mollis id metus a feugiat. Fusce velit metus, tincidunt sit amet semper sit amet, rhoncus id mi. Maecenas efficitur pharetra dui non dignissim. Aliquam vulputate vel nibh hendrerit malesuada. Nam odio augue, congue nec maximus ac, tincidunt nec ligula. In at diam in nisi tincidunt finibus at sit amet justo. Vivamus vestibulum, metus at molestie rhoncus, lectus enim cursus arcu, at pharetra velit nulla ac est. Sed at semper velit, mattis facilisis felis. Praesent varius faucibus mi ut vulputate. Vestibulum vehicula dapibus leo, a finibus magna condimentum fringilla. Morbi tempor vel ipsum in porta.
"""

# Tokenisasi teks menjadi kata-kata
tokens = word_tokenize(text)

# Menghitung frekuensi kemunculan kata-kata
freq_dist = FreqDist(tokens)

# Menampilkan 5 kata dengan frekuensi tertinggi
print("5 Kata dengan Frekuensi Tertinggi:")
for word, frequency in freq_dist.most_common(5):
    print(f'{word}: {frequency}')

from textblob import TextBlob

# Contoh teks
text_2 = """
Dick
"""

# Menganalisis sentimen teks
blob = TextBlob(text_2)
sentiment_score = blob.sentiment.polarity

# Menampilkan hasil analisis sentimen
if sentiment_score > 0:
  print("Sentimen: Positif")
elif sentiment_score < 0:
  print("Sentimen: Negatif")
else:
  print("Sentimen: Netral")

!pip install PySastrawi

"""**Parser Portal Berita Untuk Sentimen**"""

!pip install feedparser
import feedparser
from textblob import TextBlob

# Ambil berita dari RSS feed BBC News
bbc_feed = feedparser.parse("http://feeds.bbci.co.uk/news/rss.xml")

# Fungsi untuk melakukan analisis sentimen pada teks
def analyze_sentiment(text):
    blob = TextBlob(text)
    sentiment_score = blob.sentiment.polarity
    if sentiment_score > 0:
        return "Positif"
    elif sentiment_score < 0:
        return "Negatif"
    else:
        return "Netral"

# Analisis sentimen untuk beberapa artikel berita
for i, entry in enumerate(bbc_feed.entries[:5], start=1):
    title = entry.title
    summary = entry.summary

    # Tampilkan judul dan ringkasan artikel
    print(f"Artikel {i}: {title}")
    print(f"Ringkasan: {summary}")

    # Analisis sentimen ringkasan artikel
    sentiment = analyze_sentiment(summary)
    print(f"Sentimen: {sentiment}")

"""**Parser Beberapa Portal Berita Sekaligus**"""

import feedparser
from textblob import TextBlob

# Ambil berita dari RSS feed Antara News
antara_feed = feedparser.parse("https://www.antaranews.com/rss/terkini")
kompas_feed = feedparser.parse("https://www.kompas.com/")
tribun_feed = feedparser.parse("https://www.tribunnews.com/rss")
detik_feed = feedparser.parse("https://detik.com/")

feeds = {
    "Antara News": antara_feed,
    "Kompas": kompas_feed,
    "Tribun": tribun_feed,
    "Detik": detik_feed
}

# Fungsi untuk melakukan analisis sentimen pada teks
def analyze_sentiment(text):
    blob = TextBlob(text)
    sentiment_score = blob.sentiment.polarity
    if sentiment_score > 0:
        return "Positif"
    elif sentiment_score < 0:
        return "Negatif"
    else:
        return "Netral"

# Analisis sentimen untuk beberapa artikel berita dari setiap sumber
for source, feed in feeds.items():
    print(f"Berita dari {source}:")
    for i, entry in enumerate(feed.entries[:5], start=1):
        title = entry.title
        summary = entry.summary
        link = entry.link

        # Tampilkan judul, ringkasan, dan link artikel
        print(f"Artikel {i}: {title}")
        print(f"Ringkasan: {summary}")
        print(f"Link: {link}")

        # Analisis sentimen ringkasan artikel
        sentiment = analyze_sentiment(summary)
        print(f"Sentimen: {sentiment}\n")

"""**Persentase Sentimen tiap RSS**"""

# Daftar portal berita beserta RSS feed-nya
news_portals = {
    "Antara News": "https://www.antaranews.com/rss/terkini",
    "Tribun": "https://www.tribunnews.com/rss",
    "CNN Indonesia": "https://www.cnnindonesia.com/nasional/rss",
    "Tempo": "https://rss.tempo.co/nasional",
    "CNBC": "https://cnbc.com/id/10000166/device/rss"
}

# Fungsi untuk memeriksa dan menyesuaikan URL RSS feed
def adjust_rss_url(feed_url):
    # Jika feed_url tidak dimulai dengan "http://" atau "https://", tambahkan "http://"
    if not feed_url.startswith(("http://", "https://")):
        feed_url = "http://" + feed_url

    # Cek apakah feed_url valid
    try:
        feedparser.parse(feed_url)
        return feed_url
    except Exception as e:
        print(f"Error saat memeriksa feed URL: {e}")
        return None

# Fungsi untuk menghitung persentase sentimen dari sebuah daftar sentimen
def calculate_sentiment_percentage(sentiment_list):
    total = len(sentiment_list)
    if total == 0:
        return 0, 0, 0

    positive_count = sentiment_list.count('positif')
    negative_count = sentiment_list.count('negatif')
    neutral_count = sentiment_list.count('netral')

    positive_percentage = (positive_count / total) * 100
    negative_percentage = (negative_count / total) * 100
    neutral_percentage = (neutral_count / total) * 100

    return positive_percentage, negative_percentage, neutral_percentage

# Fungsi untuk mengambil sentimen dari sebuah ringkasan berita (fungsi ini harus disesuaikan dengan metode analisis sentimen yang Anda gunakan)
def analyze_sentiment(summary):
    # Contoh: fungsi ini mengembalikan sentimen secara acak untuk tujuan demonstrasi
    import random
    sentiments = ['positif', 'negatif', 'netral']
    return random.choice(sentiments)

# List untuk menyimpan sentimen dari setiap portal
sentiments = {portal: [] for portal in news_portals}

# Mengambil dan menganalisis sentimen dari setiap portal
for portal, feed_url in news_portals.items():
    adjusted_url = adjust_rss_url(feed_url)
    if adjusted_url:
        feed = feedparser.parse(adjusted_url)
        for entry in feed.entries[:500]:
            summary = entry.summary
            sentiment = analyze_sentiment(summary)
            sentiments[portal].append(sentiment)
    else:
        del news_portals[portal]  # Hapus portal yang feed-nya tidak valid

# Menampilkan hasil perbandingan sentimen
print("Perbandingan Sentimen Berita (dalam persentase):\n")
for portal, sentiment_list in sentiments.items():
    positive_percentage, negative_percentage, neutral_percentage = calculate_sentiment_percentage(sentiment_list)
    print(f"{portal}:")
    print(f"Positif: {positive_percentage:.2f}%")
    print(f"Negatif: {negative_percentage:.2f}%")
    print(f"Netral: {neutral_percentage:.2f}%")
    print()

import matplotlib.pyplot as plt

# Fungsi untuk membuat pie chart horizontal dari persentase sentimen
def plot_sentiment_horizontal_pie_chart(positive_percentage, negative_percentage, neutral_percentage, portal, index):
    labels = ['Positif', 'Negatif', 'Netral']
    sizes = [positive_percentage, negative_percentage, neutral_percentage]
    colors = ['#ff9999','#66b3ff','#99ff99']
    explode = (0.1, 0, 0)  # Pemisahan potongan untuk sentimen positif

    plt.subplot(1, 5, index + 1)  # Menetapkan subplot untuk setiap portal
    plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 10})
    plt.title(f"Sentimen Berita - {portal}")
    plt.axis('equal')  # Agar lingkaran menjadi lingkaran sejati

# Fungsi untuk menghitung persentase sentimen dari sebuah daftar sentimen
def calculate_sentiment_percentage(sentiment_list):
    total = len(sentiment_list)
    if total == 0:
        return 0, 0, 0

    positive_count = sentiment_list.count('positif')
    negative_count = sentiment_list.count('negatif')
    neutral_count = sentiment_list.count('netral')

    positive_percentage = (positive_count / total) * 100
    negative_percentage = (negative_count / total) * 100
    neutral_percentage = (neutral_count / total) * 100

    return positive_percentage, negative_percentage, neutral_percentage

# Menampilkan pie chart horizontal untuk setiap portal berita
plt.figure(figsize=(20, 5))  # Ukuran gambar diperbesar untuk menampung semua pie chart
for index, (portal, sentiment_list) in enumerate(sentiments.items()):
    positive_percentage, negative_percentage, neutral_percentage = calculate_sentiment_percentage(sentiment_list)
    plot_sentiment_horizontal_pie_chart(positive_percentage, negative_percentage, neutral_percentage, portal, index)

plt.tight_layout()  # Menyesuaikan jarak antar subplot
plt.show()

import datetime
import requests
from bs4 import BeautifulSoup
from flair.models import TextClassifier
from flair.data import Sentence
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from collections import Counter
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional

# Load model sentimen Flair
classifier = TextClassifier.load('en-sentiment')

# Load model sentimen dan subjek-objek dari Hugging Face Transformers
sentiment_pipeline = pipeline("sentiment-analysis", model="cahya/distilbert-base-indonesian")
ner_pipeline = pipeline("ner", model="cahya/distilbert-base-indonesian")
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')
indobert_model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1')

# Download corpus untuk POS tagging nltk
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

# Fungsi untuk mengambil teks artikel dan metadata dari link berita
def scrape_article_data(link):
    try:
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Mengambil teks artikel
        article_content = soup.find('div', class_='detail__body-text')
        article_text = article_content.get_text(separator=' ').strip() if article_content else None

        # Mengambil metadata
        author_element = soup.find('span', class_='author__name')
        author = author_element.text.strip() if author_element else "Unknown Author"

        date_element = soup.find('div', class_='date')
        upload_date = datetime.datetime.strptime(date_element['date'], "%Y-%m-%d %H:%M:%S") if date_element else datetime.date.today()

        return article_text, author, upload_date
    except Exception as e:
        print(f"Error saat scraping data dari link: {e}")
        return None, None, None

# Fungsi untuk menganalisis artikel dan menampilkan informasi
def analyze_article(article_text, author, upload_date):
    # Stemming teks artikel menggunakan Sastrawi
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    article_text_stemmed = stemmer.stem(article_text)

    # Analisis sentimen menggunakan Flair
    sentence = Sentence(article_text_stemmed)
    classifier.predict(sentence)
    flair_sentiment = sentence.labels[0].value

    # Analisis sentimen menggunakan IndoBERT
    inputs = tokenizer(article_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = indobert_model(**inputs)
    indobert_predicted_class = outputs.logits.argmax().item()
    indobert_sentiment = "Positive" if indobert_predicted_class == 1 else "Negative"

    # Tokenisasi kata menggunakan NLTK
    words = word_tokenize(article_text)
    # Filter tanda baca dari kata-kata yang sering muncul
    words = [word for word in words if word.isalnum()]
    # POS Tagging menggunakan NLTK
    tagged_words = pos_tag(words)

    # Memisahkan subjek, objek, dan nama orang menggunakan model TensorFlow
    subjects, objects, names = extract_entities(tagged_words)

    # Menghitung persentase kata sentimen
    positive_words = re.findall(r'\b(?:baik|bagus|positif)\b', article_text, flags=re.IGNORECASE)
    negative_words = re.findall(r'\b(?:buruk|negatif)\b', article_text, flags=re.IGNORECASE)
    total_words = len(words)
    positive_percentage_flair = (len(positive_words) / total_words) * 100
    negative_percentage_flair = (len(negative_words) / total_words) * 100
    neutral_percentage_flair = 100 - positive_percentage_flair - negative_percentage_flair

    # Menampilkan informasi artikel
    print("\n" + "="*50)
    print("Analisis Artikel:")
    print("="*50)
    print(f"Tanggal Unggah: {upload_date}")
    print(f"Penulis: {author}")
    print("="*50)
    print("Ringkasan Artikel:")
    print("-"*50)
    print(article_text[:300] + '...' if len(article_text) > 300 else article_text)
    print("="*50)
    print("Informasi Artikel:")
    print("-"*50)
    print(f"Sentimen (Flair): {flair_sentiment}")
    print(f"Sentimen (IndoBERT): {indobert_sentiment}")
    print("-"*50)
    print("Persentase Sentimen:")
    print("-"*50)
    print("Flair:")
    print(f"Positif: {positive_percentage_flair:.2f}%")
    print(f"Negatif: {negative_percentage_flair:.2f}%")
    print(f"Netral: {neutral_percentage_flair:.2f}%")
    print("-"*50)
    print("Subjek:")
    print("-"*50)
    print(", ".join(subjects))
    print("-"*50)
    print("Objek:")
    print("-"*50)
    print(", ".join(objects))
    print("-"*50)
    print("Nama Orang:")
    print("-"*50)
    print(", ".join(names))
    print("-"*50)

# Fungsi untuk memisahkan subjek, objek, dan nama orang menggunakan model TensorFlow
def extract_entities(tagged_words):
    subjects = []
    objects = []
    names = []

    for word, tag in tagged_words:
        if tag.startswith('NN'):  # Subjek atau objek
            if word.istitle():  # Nama orang
                names.append(word)
            elif tag == 'NNP':  # Kata benda tunggal (proper noun)
                subjects.append(word)
            else:
                objects.append(word)

    return subjects, objects, names

# Tiga link berita contoh
article_links = [
    "https://news.detik.com/pemilu/d-7239048/prabowo-gibran-unggul-di-sultra-disusul-amin-dan-ganjar-mahfud",
    "https://news.detik.com/berita/d-7238960/catat-ini-daftar-19-kota-kabupaten-tujuan-mudik-gratis-pemprov-dki",
    "https://news.detik.com/bbc-world/d-7238924/kesaksian-petugas-medis-gaza-yang-mengaku-disiksa-tentara-israel"
]

# Menganalisis dan menampilkan informasi dari tiga artikel
for i, article_link in enumerate(article_links):
    print(f"\nAnalisis Artikel {i+1}:")
    article_text, author, upload_date = scrape_article_data(article_link)
    if article_text:
        analyze_article(article_text, author, upload_date)
    else:
        print("Tidak dapat mengambil teks artikel dari link.")

